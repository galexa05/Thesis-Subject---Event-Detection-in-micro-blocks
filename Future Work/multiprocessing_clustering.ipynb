{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "import json\n",
    "#%matplotlib inline\n",
    "#pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "import hdbscan\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import textacy\n",
    "\n",
    "from gmplot import gmplot\n",
    "from mapsplotlib import mapsplot as mplt\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tag import pos_tag_sents\n",
    "from pymprog import *\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "from time import strptime\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"relevant_json.json\"\n",
    "list_tweets = []\n",
    "count = 0 \n",
    "sum = 1000\n",
    "with open(fname,encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "        except:\n",
    "            pass\n",
    "        list_tweets.append(tweet)\n",
    "#         count+=1\n",
    "#         if count==sum:\n",
    "#             print(count)\n",
    "#             sum*=2\n",
    "# list_tweets[0].keys()\n",
    "\n",
    "df_relevant = pd.DataFrame(data=[tweet[\"full_text\"] for tweet in list_tweets], columns=['tweets'])\n",
    "df_relevant['created_at'] = np.array([tweet[\"created_at\"] for tweet in list_tweets])\n",
    "df_relevant['id'] = np.array([int(tweet[\"id_str\"]) for tweet in list_tweets])\n",
    "df_relevant['rt'] = np.array([tweet[\"retweeted\"] for tweet in list_tweets])\n",
    "df_relevant['lang'] = np.array([tweet[\"lang\"] for tweet in list_tweets])\n",
    "df_relevant['Predicted_Event'] = -1\n",
    "\n",
    "# df_relevant.shape[0]\n",
    "\n",
    "fname = \"relevant_tweets.tsv\"\n",
    "event_id = pd.read_csv(fname, sep='\\t',header=None,names=['Event','id'])\n",
    "\n",
    "# event_id.shape[0]\n",
    "\n",
    "df_relevant = df_relevant.merge(event_id, left_on='id', right_on='id')\n",
    "df_relevant.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['tweets', 'created_at', 'id','rt','lang'] \n",
    "df_irrelevant = pd.read_csv('output_1.csv', names=colnames, header=None)\n",
    "df_irrelevant['Predicted_Event'] = -1\n",
    "df_irrelevant['Event'] = -1\n",
    "print(df_irrelevant.shape[0])\n",
    "df_irrelevant.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_relevant,df_irrelevant]\n",
    "df = pd.concat(frames)\n",
    "# df\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plots number of tweets in each language\n",
    "counts = df['lang'].value_counts()\n",
    "plt.bar(counts.index, counts, align='center', alpha=0.5)\n",
    "plt.xlabel('Tweet Language')\n",
    "plt.ylabel('# of Tweets')\n",
    "plt.title('Tweets 2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Old df shape:\",df.shape[0])\n",
    "df = df.loc[df[\"lang\"]=='en']\n",
    "print(\"New df shape:\",df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "\n",
    "def create_date(text):\n",
    "    temp = text.split(\" \")\n",
    "    x = str(datetime.datetime(int(temp[5]),int(strptime(temp[1],'%b').tm_mon),int(temp[2]))).split()[0]\n",
    "    return x\n",
    "\n",
    "df['date'] = df['created_at'].apply(lambda x: create_date(x))\n",
    "df['time'] =  np.array([tweet.split()[3] for tweet in df[\"created_at\"]])\n",
    "df['Datetime']= pd.to_datetime(df['date'].apply(str)+' '+df['time'].apply(lambda x: x.split(':')[0]) + df['time'].apply(lambda x: x.split(':')[1]))\n",
    "df['DateHour'] = pd.to_datetime(df['date'].apply(str)+' '+df['time'].apply(lambda x: x.split(':')[0])+':00')\n",
    "\n",
    "tableFlag=[]\n",
    "data_sorted = df.sort_values(by=['DateHour'],inplace=False)\n",
    "data_sorted = data_sorted.reset_index(drop=True)\n",
    "\n",
    "list_date = []\n",
    "for temp in data_sorted[\"DateHour\"]:\n",
    "    list_date.append(temp)\n",
    "\n",
    "list_date = list(dict.fromkeys(list_date))\n",
    "print(len(list_date))\n",
    "# list_date\n",
    "timeseries_y = []\n",
    "for i in range(0,len(list_date)):\n",
    "    timeseries_y.append(df.loc[df[\"DateHour\"]==list_date[i]].shape[0])\n",
    "plt.plot(list_date,timeseries_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "global count10\n",
    "# result = []\n",
    "# article = df[\"tweets\"][0]\n",
    "# spacy_nlp = spacy.load('en')\n",
    "# document = spacy_nlp(article)\n",
    "\n",
    "# useful_entities = [u'NORP', u'PERSON', u'FACILITY', u'ORG', u'GPE', u'LOC', u'EVENT', u'DATE', u'TIME',u'NUM']\n",
    "# for element in document.ents:\n",
    "#     if (element.label_ in useful_entities):\n",
    "#         result.append(element)\n",
    "# print('Original Sentence: %s' % (article))\n",
    "# for element in document.ents:\n",
    "#     print('Type: %s, Value: %s' % (element.label_, element))\n",
    "# print(result)\n",
    "\n",
    "def get_NER(tweet):\n",
    "    result = []\n",
    "    spacy_nlp = spacy.load('en')\n",
    "    document = spacy_nlp(tweet)\n",
    "    useful_entities = [u'NORP', u'PERSON', u'FACILITY', u'ORG', u'GPE', u'LOC', u'EVENT', u'DATE', u'TIME',u'NUM']\n",
    "    for element in document.ents:\n",
    "        if (element.label_ in useful_entities):\n",
    "            result.append(str(element))\n",
    "#     print(\"hello\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfchange = df.loc[df[\"DateHour\"]==list_date[0]]\n",
    "# dfchange = dfchange[0:4]\n",
    "# dfchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get unique values \n",
    "def getNumClusters(list1): \n",
    "  \n",
    "    # intilize a null list \n",
    "    unique_list = [] \n",
    "      \n",
    "    # traverse for all elements \n",
    "    for x in list1: \n",
    "        # check if exists in unique_list or not \n",
    "        if x not in unique_list: \n",
    "            unique_list.append(x) \n",
    "    return len(unique_list)\n",
    "\n",
    "# take second element for sort\n",
    "def takeSecond(elem):\n",
    "    return elem[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from multiprocessing import Pool\n",
    "\n",
    "num_partitions = 4 #number of partitions to split dataframe\n",
    "num_cores = 4 #number of cores on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fun(data_pd):\n",
    "    data_pd['ner'] = data_pd[\"tweets\"].map(lambda x: get_NER(x))\n",
    "    return data_pd\n",
    "\n",
    "def parallelize_dataframe(df, my_func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(my_func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "final_df = parallelize_dataframe(dfchange,my_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(final_df[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def getTfIdf(clean_train,windows_corpus):\n",
    "    minimum = max(int(windows_corpus * 0.0025), 10)\n",
    "    print(minimum)\n",
    "    tfidf_vectorizer = TfidfVectorizer(#min_df=0.5,\n",
    "                                       min_df=minimum,\n",
    "                                       #max_df=0.8,\n",
    "                                       max_features=2000000,\n",
    "                                       stop_words='english',\n",
    "                                       use_idf=True,\n",
    "                                       ngram_range=(1,3)\n",
    "    )\n",
    "    # Tf-idf-weighted term-document sparse matrix\n",
    "    try :\n",
    "        tfidf_train_data_features = tfidf_vectorizer.fit_transform(clean_train)\n",
    "#         print(tfidf_vectorizer.get_feature_names())\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "\n",
    "    #sim = (tfidf_train_data_features * tfidf_train_data_features.T).A\n",
    "    return tfidf_train_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTweetsDBSCAN(tfidf,e):\n",
    "    clustering = DBSCAN(eps=e, min_samples=10,n_jobs=-1).fit(tfidf)\n",
    "    #clustering = hdbscan.HDBSCAN(min_cluster_size=10).fit(tfidf)\n",
    "\n",
    "    clusters = clustering.labels_.tolist()\n",
    "    temp = clusters.copy()\n",
    "    temp.remove\n",
    "    \n",
    "    addedCluster = tweets.copy()\n",
    "    addedCluster['Cluster'] = clusters\n",
    "    num_clusters = getNumClusters(clusters)\n",
    "\n",
    "    pd.options.display.max_colwidth = 100\n",
    "\n",
    "    cls = []\n",
    "    #num_clusters = 55\n",
    "    max_num_cluster = 0\n",
    "\n",
    "    for i in range(0, num_clusters):\n",
    "        tweetsInCluster = addedCluster[addedCluster['Cluster'] == i]\n",
    "        if(tweetsInCluster.shape[0] > 0):\n",
    "            cls.append((tweetsInCluster,tweetsInCluster.shape[0]))\n",
    "            \n",
    "    cls.sort(key=takeSecond,reverse=True)\n",
    "    clusters = []\n",
    "    for i in range(len(cls)):\n",
    "        clusters.append(cls[i][0])\n",
    "        \n",
    "    #Noise Data\n",
    "    noise_data = addedCluster[addedCluster['Cluster'] == -1]\n",
    "#     print(\"Size:\",noise_data.shape[0])\n",
    "#     print(noise_data[\"tweets\"],noise_data[\"Event\"])\n",
    "    \n",
    "    return {\"clusters\":clusters,\"noise_data\":noise_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert   \n",
    "def listToString(s):\n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    # return string   \n",
    "    return (str1.join(s)) \n",
    "\n",
    "final_df[\"total_sentence\"] = np.array([listToString(temp) for temp in final_df['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = final_df\n",
    "tweetsContent = tweets.copy()[\"total_sentence\"]\n",
    "tfidf = getTfIdf(tweetsContent,dfchange.shape[0])\n",
    "print(\"Event\",count,\":\")\n",
    "count+=1\n",
    "print(\"Tweets: \",dfchange.shape[0])\n",
    "x_object = findTweetsDBSCAN(tfidf,0.8)\n",
    "cluster = x_object[\"clusters\"]\n",
    "noise_data = x_object[\"noise_data\"]\n",
    "\n",
    "for i in range(0,len(cluster)):\n",
    "    temp = cluster[i].set_index([\"Event\",\"id\"]).count(level=\"Event\")\n",
    "    temp2 = temp.loc[temp[\"Cluster\"]==temp[\"Cluster\"].max()]\n",
    "    predicted_event = temp2.index[0]\n",
    "    \n",
    "    for index,row in cluster[i].iterrows():\n",
    "        #row[\"Predicted_Event\"] = predicted_event\n",
    "        list_y.append(row[\"Event\"])\n",
    "        list_y_preticted.append(predicted_event)\n",
    "#     print(\"Predicted Event:\", -1)\n",
    "\n",
    "for i in range(0,len(cluster)):\n",
    "    print(print(\"Cluster\", i, \":\", cluster[i].shape[0]))\n",
    "    print(cluster[i][\"tweets\"],cluster[i][\"Event\"])\n",
    "    sum+= cluster[i].shape[0]\n",
    "    #print(\"\\n\")\n",
    "\n",
    "# print(\"Noise data:\",noise_data[\"tweets\"],noise_data[\"Event\"])\n",
    "for index,row in noise_data.iterrows():\n",
    "    list_y.append(row[\"Event\"])\n",
    "    list_y_preticted.append(-1)\n",
    "\n",
    "print(\"Number of clusters\", len(cluster))\n",
    "print(\"Total tweets that clustered\",sum)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "print(accuracy_score(list_y,list_y_preticted))\n",
    "print(f1_score(list_y,list_y_preticted,average='micro'))\n",
    "print(adjusted_rand_score(list_y,list_y_preticted))\n",
    "print(adjusted_mutual_info_score(list_y,list_y_preticted))\n",
    "print(normalized_mutual_info_score(list_y, list_y_preticted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
