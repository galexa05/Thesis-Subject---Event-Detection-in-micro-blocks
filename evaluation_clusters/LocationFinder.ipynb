{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "#%matplotlib inline\n",
    "#pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "import hdbscan\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import textacy\n",
    "import csv\n",
    "import sklearn\n",
    "\n",
    "from gmplot import gmplot\n",
    "from mapsplotlib import mapsplot as mplt\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tag import pos_tag_sents\n",
    "from pymprog import *\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "from time import strptime\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "import clustering_models as cl\n",
    "import timestamp_graphs as tg\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from geograpy import extraction\n",
    "from geograpy import places\n",
    "import collections\n",
    "from geograpy import places\n",
    "from urllib.request import urlopen\n",
    "import data_preprocessing_methods as dpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106017\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>rt</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>Event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News: Chinese author Mo Yan wins Nobel http://...</td>\n",
       "      <td>Thu Oct 11 11:08:32 +0000 2012</td>\n",
       "      <td>256350567369175040</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Eindhoven</td>\n",
       "      <td>{\"lat\": 51.47601928708412, \"lon\": 5.4647688255...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  \\\n",
       "0  News: Chinese author Mo Yan wins Nobel http://...   \n",
       "\n",
       "                       created_at                  id     rt lang   location  \\\n",
       "0  Thu Oct 11 11:08:32 +0000 2012  256350567369175040  False   en  Eindhoven   \n",
       "\n",
       "                                         coordinates  Event  \n",
       "0  {\"lat\": 51.47601928708412, \"lon\": 5.4647688255...      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colnames=['tweets', 'created_at', 'id','rt','lang','location','coordinates','Event'] \n",
    "df = pd.read_csv('relevant_with_coordinates.csv', names=colnames, header=None)\n",
    "print(df.shape[0])\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['location', 'coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import data_preprocessing_methods as dpm\n",
    "temp = np.vectorize(dpm.remove_patterns)(df['tweets'],\"@[\\w]*\",\"#[\\w]*\",\"RT\")\n",
    "df[\"text\"] = np.array([text[\"input_text\"] for text in temp])\n",
    "df[\"hashtag\"] = np.array([dpm.list_lower(text[\"hashtags\"]) for text in temp])\n",
    "df[\"user_mentions\"] = np.array([text[\"user_mentions\"] for text in temp]) \n",
    "df[\"RT\"] = np.array([text[\"is_RT\"] for text in temp])\n",
    "df[\"URL\"] = np.array([text[\"is_URL\"] for text in temp])\n",
    "df['Length_URL'] = np.array([len(temp) for temp in df['URL']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31361"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df.loc[df['Length_URL']!=0]\n",
    "temp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPlaces(list_url):\n",
    "    list_places = []\n",
    "    for temp_url in list_url:\n",
    "        try:\n",
    "            ret = urlopen(temp_url)\n",
    "            e = extraction.Extractor(url=ret.url)\n",
    "            e.find_entities()\n",
    "            name_places = e.places\n",
    "            counter = collections.Counter(name_places)\n",
    "            crucial_words = counter.most_common(3)\n",
    "            for i in range(len(crucial_words)):\n",
    "                list_places.append(crucial_words[i][0])\n",
    "#             print(temp_url)\n",
    "        except:\n",
    "            pass\n",
    "#     print(list_places)\n",
    "    return list_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [getPlaces(temp) for temp in df['URL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df['Places'] = np.array([getPlaces(temp) for temp in df['URL']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(r'relevant_with_place.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geograpy\n",
    "# url = 'http://t.co/9QFHaJyE'\n",
    "# places = geograpy.get_place_context(url=url)\n",
    "from geograpy import extraction\n",
    "from urllib.request import urlopen\n",
    "try:\n",
    "    ret = urlopen('https://paper.li/tppowers/Advertising#/')\n",
    "    e = extraction.Extractor(url=ret.url)\n",
    "    e.find_entities()\n",
    "\n",
    "    # You can now access all of the places found by the Extractor\n",
    "    print(e.places)\n",
    "    from geograpy import places\n",
    "    pc = places.PlaceContext(e.places)\n",
    "    pc.set_countries()\n",
    "    print(pc.countries) #['United States']\n",
    "    pc.country_mentions\n",
    "except:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413986\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>rt</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love ms.sivney class (Sports &amp;amp; Entertaim...</td>\n",
       "      <td>Wed Oct 31 16:48:39 +0000 2012</td>\n",
       "      <td>263683917889040400</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Macedonia</td>\n",
       "      <td>{'lat': 41.82038761113906, 'lon': 22.008566323...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  \\\n",
       "0  I love ms.sivney class (Sports &amp; Entertaim...   \n",
       "\n",
       "                       created_at                  id     rt lang   location  \\\n",
       "0  Wed Oct 31 16:48:39 +0000 2012  263683917889040400  False   en  Macedonia   \n",
       "\n",
       "                                         coordinates  \n",
       "0  {'lat': 41.82038761113906, 'lon': 22.008566323...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colnames=['tweets', 'created_at', 'id','rt','lang','location','coordinates'] \n",
    "df_irrelevant = pd.read_csv('irrelevant_with_coordinates.csv', names=colnames, header=None)\n",
    "print(df_irrelevant.shape[0])\n",
    "df_irrelevant.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.vectorize(dpm.remove_patterns)(df_irrelevant['tweets'],\"@[\\w]*\",\"#[\\w]*\",\"RT\")\n",
    "df_irrelevant[\"text\"] = np.array([text[\"input_text\"] for text in temp])\n",
    "df_irrelevant[\"hashtag\"] = np.array([dpm.list_lower(text[\"hashtags\"]) for text in temp])\n",
    "df_irrelevant[\"RT\"] = np.array([text[\"is_RT\"] for text in temp])\n",
    "df_irrelevant[\"URL\"] = np.array([text[\"is_URL\"] for text in temp])\n",
    "df_irrelevant['Length_URL'] = np.array([len(temp) for temp in df_irrelevant['URL']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irrelevant['Places'] = np.array([getPlaces(temp) for temp in df['URL']])\n",
    "df.to_csv(r'irrelevant_with_place.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
